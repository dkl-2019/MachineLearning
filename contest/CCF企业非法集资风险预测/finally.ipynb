{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cab\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,precision_recall_fscore_support,roc_curve,auc,roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#from featexp import get_univariate_plots#用于特征筛选，需要先安装featexp\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['font.sans-serif']=['Simhei']\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "import json\n",
    "import jieba\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "entprise_train = pd.read_csv('train/entprise_info.csv',engine='python')\n",
    "base_info = pd.read_csv('train/base_info.csv',engine='python',encoding='utf-8')\n",
    "entprise_test = pd.read_csv('entprise_evaluate.csv')\n",
    "reccap_predict = pd.read_csv('reccap_predict.csv')['reccap']\n",
    "empnum_predict = pd.read_csv('empnum_predict.csv')['empnum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_info=pd.read_csv('train/base_info.csv')#企业的基本信息\n",
    "annual_report_info=pd.read_csv('train/annual_report_info.csv')#企业的年报基本信息\n",
    "tax_info=pd.read_csv('train/tax_info.csv')#企业的纳税信息\n",
    "change_info=pd.read_csv('train/change_info.csv')#变更信息\n",
    "news_info=pd.read_csv('train/news_info.csv')#舆情信18\n",
    "other_info=pd.read_csv('train/other_info.csv')#其它信息\n",
    "entprise_info=pd.read_csv('train/entprise_info.csv')#企业标注信息{0: 13884, 1: 981}\n",
    "entprise_evaluate=pd.read_csv('entprise_evaluate.csv')#未标注信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change_info\n",
    "change_info_clean = change_info.groupby('id',sort=False).agg('mean')\n",
    "change_info['bgrq'] = (change_info['bgrq'] / 10000000000).astype(int)\n",
    "change_info_df = change_info.groupby('id').agg({\n",
    "    'bgxmdm': ['count', 'nunique'],\n",
    "    'bgq':['nunique'],\n",
    "    'bgh': ['nunique'],\n",
    "    'bgrq': ['nunique'],\n",
    "})\n",
    "change_info_df.columns = pd.Index(['changeinfo_' + e[0] + \"_\" + e[1].upper() \n",
    "                                for e in change_info_df.columns.tolist()])\n",
    "change_info_df = change_info_df.reset_index()\n",
    "change_info = pd.merge(change_info_df,change_info_clean,how='left',on='id')\n",
    "#other_info\n",
    "#空值大于0.5的列都删除掉\n",
    "other_info = other_info[~other_info['id'].duplicated()]\n",
    "other_info['other_SUM'] = other_info[['legal_judgment_num', 'brand_num', 'patent_num']].sum(1)\n",
    "other_info['other_NULL_SUM'] = other_info[['legal_judgment_num', 'brand_num', 'patent_num']].isnull().astype(int).sum(1)\n",
    "#news_info\n",
    "news_info_clean=news_info.drop(['public_date'],axis=1)\n",
    "#对object类型进行编码\n",
    "news_info_clean['positive_negtive']=news_info_clean['positive_negtive'].fillna(\"中立\")\n",
    "#\n",
    "dic={}\n",
    "cate=news_info_clean.positive_negtive.unique()\n",
    "for i in range(len(cate)):\n",
    "    dic[cate[i]]=i\n",
    "#\n",
    "news_info_clean['positive_negtive']=news_info_clean['positive_negtive'].map(dic)\n",
    "news_info_clean = news_info_clean.groupby('id',sort=False).agg('mean')\n",
    "news_info_clean=pd.DataFrame(news_info_clean).reset_index()\n",
    "news_info['public_date'] = news_info['public_date'].apply(lambda x: x if '-' in str(x) else np.nan)\n",
    "news_info['public_date'] = pd.to_datetime(news_info['public_date'])\n",
    "news_info['public_date'] = (datetime.now() - news_info['public_date']).dt.days\n",
    "\n",
    "news_info_df = news_info.groupby('id').agg({'public_date': ['count','max','min','mean']}).reset_index()\n",
    "news_info_df.columns = ['id', 'public_date_COUNT', 'public_MAX', 'public_MIN', 'public_MEAN']\n",
    "news_info_df2 = pd.pivot_table(news_info, index='id', columns='positive_negtive', aggfunc='count').reset_index()\n",
    "news_info_df2.columns = ['id', 'news_COUNT1', 'news_COUNT2', 'news_COUNT3']\n",
    "news_info_df = pd.merge(news_info_df, news_info_df2)\n",
    "news_info_1 = pd.merge(news_info_df,news_info_clean,how='left')\n",
    "#处理annual_report_info的数据\n",
    "#空值大于0.5的列都删除掉\n",
    "annual_report_info_clean=annual_report_info.dropna(thresh=annual_report_info.shape[0]*0.5,how='all',axis=1)\n",
    "#对object类型进行编码\n",
    "annual_report_info_clean['BUSSTNAME']=annual_report_info_clean['BUSSTNAME'].fillna(\"无\")\n",
    "dic = {'无':-1,'开业':0, '歇业':1, '停业':2, '清算':3}\n",
    "#\n",
    "annual_report_info_clean['BUSSTNAME']=annual_report_info_clean['BUSSTNAME'].map(dic)\n",
    "annual_report_info_clean = annual_report_info_clean.groupby('id',sort=False).agg('mean')\n",
    "annual_report_info_clean=pd.DataFrame(annual_report_info_clean).reset_index()\n",
    "#处理tax数据\n",
    "tax_info_clean=tax_info.copy()\n",
    "tax_info_clean['START_DATE']=pd.to_datetime(tax_info_clean['START_DATE'])\n",
    "tax_info_clean['END_DATE']=pd.to_datetime(tax_info_clean['END_DATE'])\n",
    "tax_info_clean['gap_day']=(tax_info_clean['END_DATE']-tax_info_clean['START_DATE']).dt.total_seconds()//3600//24\n",
    "tax_info_clean=tax_info_clean.drop(['START_DATE','END_DATE'],axis=1)\n",
    "tax_info_clean['TAX_CATEGORIES']=tax_info_clean['TAX_CATEGORIES'].fillna(\"无\")#17 unique\n",
    "tax_info_clean['TAX_ITEMS']=tax_info_clean['TAX_ITEMS'].fillna(\"无\")#275 TAX_ITEMS\n",
    "#对object类型进行编码\n",
    "dic={}\n",
    "cate=tax_info_clean.TAX_CATEGORIES.unique()\n",
    "for i in range(len(cate)):\n",
    "    dic[cate[i]]=i\n",
    "tax_info_clean['TAX_CATEGORIES']=tax_info_clean['TAX_CATEGORIES'].map(dic)\n",
    "#\n",
    "dic={}\n",
    "cate=tax_info_clean.TAX_ITEMS.unique()\n",
    "for i in range(len(cate)):\n",
    "    dic[cate[i]]=i\n",
    "tax_info_clean['TAX_ITEMS']=tax_info_clean['TAX_ITEMS'].map(dic)\n",
    "tax_info_clean['income']=tax_info_clean['TAX_AMOUNT']/tax_info_clean['TAX_RATE']\n",
    "#\n",
    "tax_info_clean = tax_info_clean.groupby('id',sort=False).agg('mean')\n",
    "tax_info_clean=pd.DataFrame(tax_info_clean).reset_index()\n",
    "#税额分箱\n",
    "tax_info_clean['TAX_AMOUNT']=tax_info_clean['TAX_AMOUNT'].fillna(tax_info_clean['TAX_AMOUNT'].median())\n",
    "tax_info_clean['bucket_TAX_AMOUNT']=pd.qcut(tax_info_clean['TAX_AMOUNT'], 10, labels=False,duplicates='drop')\n",
    "print('finished .............')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_info_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "entprise = pd.concat([entprise_train,entprise_test])\n",
    "data = pd.merge(entprise,base_info,on='id')\n",
    "data.drop(['ptbusscope','midpreindcode','score'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.merge(annual_report_info_clean,how='outer')\n",
    "data=data.merge(tax_info_clean,how='outer')\n",
    "data=data.merge(change_info,how='outer')\n",
    "data=data.merge(news_info_1,how='outer',on='id')\n",
    "data=data.merge(other_info,how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['enttypeitem'] = data['enttypegb']//10*10\n",
    "data['enttypeminu'] = data['enttypegb']\n",
    "data['regcap'] = data['regcap'].fillna(data['regcap'].mean())\n",
    "data['reccap'] = reccap_predict\n",
    "data['empnum'] = empnum_predict\n",
    "data['reccap'] = data['reccap']\n",
    "data['opform'].fillna(10,inplace=True)\n",
    "data['opform'] = data['opform'].astype(str)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder1 = LabelEncoder()\n",
    "labelencoder2 = LabelEncoder()\n",
    "labelencoder3 = LabelEncoder()\n",
    "labelencoder4 = LabelEncoder()\n",
    "data['opform'] = labelencoder1.fit_transform(data['opform'])\n",
    "data['oploc'] = labelencoder2.fit_transform(data['oploc'])\n",
    "data['industryphy'] = labelencoder3.fit_transform(data['industryphy'])\n",
    "data['oplocdistrict'] = labelencoder4.fit_transform(data['oplocdistrict'])\n",
    "def opto_process(x):\n",
    "    if len(x)>3:\n",
    "        return x[:4]\n",
    "    else:\n",
    "        return x\n",
    "data['opfrom'] = data['opfrom'].map(lambda x:x[:4])\n",
    "data['opto'].fillna('nan',inplace=True)\n",
    "data['opto'] = data['opto'].map(opto_process)\n",
    "data['opfrom'] = data['opfrom'].astype(int)\n",
    "data['opto'] = data['opto'].astype(float)\n",
    "data['industryco'] = data['industryco'].fillna(data['industryco'].mean())\n",
    "data.loc[data['opto'].isnull(),\"opto\"] = data[data['opto'].isnull()]['opfrom']+50.0\n",
    "data['opscope'] = data['opscope'].apply(lambda x:x.replace(\"、\",\"/\").replace(\"；\",\"/\").replace(\"，\",\"/\").replace(\"。\",\"/\"))\n",
    "data['opscope'] = data['opscope'].apply(lambda x:x.replace(\"（\",\"/\").replace(\"）\",\"/\").replace(\"，\",\"/\").replace(\"。\",\"/\"))\n",
    "data['opscope_1'] = data['opscope'].apply(lambda x:x.split(\"/\"))\n",
    "data['opscope_feature'] = data['opscope'].apply(lambda x:x.split(\"/\")[0][-2:])\n",
    "le2 = LabelEncoder()\n",
    "data['opscope_feature'] = le2.fit_transform(data['opscope_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['enttypeitem_RANK'] = data['enttypeitem'].map(data.groupby(['enttypeitem'])['label'].mean().rank())\n",
    "# data['enttypeminu_RANK'] = data['enttypeminu'].map(data.groupby(['enttypeminu'])['label'].mean().rank())\n",
    "# data['enttype_RANK'] = data['enttype'].map(data.groupby(['enttype'])['label'].mean().rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分桶完毕.................\n"
     ]
    }
   ],
   "source": [
    "def bucket(name,bucket_len):\n",
    "    gap_list=[data[name].quantile(i/bucket_len) for i in range(bucket_len+1)]\n",
    "    len_data=len(data[name])\n",
    "    new_col=[]\n",
    "    for i in data[name].values:\n",
    "        for j in range(len(gap_list)):\n",
    "            if gap_list[j]>=i:\n",
    "                encode=j\n",
    "                break\n",
    "        new_col.append(encode)\n",
    "    return new_col\n",
    "#注册资本_实缴资本\n",
    "data['regcap_reccap']=data['regcap']-data['reccap']\n",
    "#注册资本分桶\n",
    "data['regcap']=data['regcap'].fillna(data['regcap'].median())\n",
    "data['bucket_regcap']=bucket('regcap',5)\n",
    "#实缴资本分桶\n",
    "data['reccap']=data['reccap'].fillna(data['reccap'].median())\n",
    "data['bucket_reccap']=bucket('reccap',5)\n",
    "#注册资本_实缴资本分桶\n",
    "data['regcap_reccap']=data['regcap_reccap'].fillna(data['regcap_reccap'].median())\n",
    "data['bucket_regcap_reccap']=bucket('regcap_reccap',5)\n",
    "print('分桶完毕.................')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data['dom'] = data['dom'].apply(lambda x:x[:16])\n",
    "le7 = LabelEncoder()\n",
    "data['dom'] = le7.fit_transform(data['dom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 24865/24865 [00:00<00:00, 132140.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 24865/24865 [00:00<00:00, 132140.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 24865/24865 [00:00<00:00, 135750.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 24865/24865 [00:00<00:00, 130749.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 24865/24865 [00:00<00:00, 130065.06it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 24865/24865 [00:00<00:00, 132847.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def cross_two(name_1,name_2):\n",
    "    new_col=[]\n",
    "    encode=0\n",
    "    dic={}\n",
    "    val_1=data[name_1]\n",
    "    val_2=data[name_2]\n",
    "    for i in tqdm(range(len(val_1))):\n",
    "        tmp=str(val_1[i])+'_'+str(val_2[i])\n",
    "        if tmp in dic:\n",
    "            new_col.append(dic[tmp])\n",
    "        else:\n",
    "            dic[tmp]=encode\n",
    "            new_col.append(encode)\n",
    "            encode+=1\n",
    "    return new_col\n",
    "#作企业类型-小类的交叉特征\n",
    "data['enttypegb']=data['enttypegb'].fillna(\"无\")\n",
    "data['enttypeitem']=data['enttypeitem'].fillna(\"无\")\n",
    "new_col=cross_two('enttypegb','enttypeitem')#作企业类型-小类的交叉特征\n",
    "data['enttypegb_enttypeitem']=new_col\n",
    "new_col=cross_two('enttype','enttypegb_enttypeitem')#作企业类型-小类的交叉特征\n",
    "data['enttype_enttypegb_enttypeitem']=new_col\n",
    "#\n",
    "#行业类别-细类的交叉特征\n",
    "data['industryphy']=data['industryphy'].fillna(\"无\")\n",
    "data['industryco']=data['industryco'].fillna(\"无\")\n",
    "new_col=cross_two('industryphy','industryco')#作企业类型-小类的交叉特征\n",
    "data['industryphy_industryco']=new_col\n",
    "#企业类型-行业类别的交叉特征\n",
    "new_col=cross_two('enttypegb','industryphy')#作企业类型-小类的交叉特征\n",
    "data['enttypegb_industryphy']=new_col\n",
    "#行业类别-企业类型小类的交叉特征\n",
    "new_col=cross_two('industryphy','enttypeitem')#作企业类型-小类的交叉特征\n",
    "data['industryphy_enttypeitem']=new_col\n",
    "#行业类别细类--企业类型小类的交叉特征\n",
    "new_col=cross_two('industryco','enttypeitem')#作企业类型-小类的交叉特征\n",
    "data['industryco_enttypeitem']=new_col\n",
    "#企业类型-小类-行业类别-细类的交叉特征\n",
    "# new_col=cross_two('enttypegb_enttypeitem','industryphy_industryco')#作企业类型-小类的交叉特征\n",
    "# data['enttypegb_enttypeitem_industryphy_industryco']=new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fea in ['industryphy','enttype','enttypeitem','enttypeminu']:\n",
    "    data[fea+'_counts'] = data.groupby([fea])['id'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[data['label'].notnull()]\n",
    "kind = x_train['label']\n",
    "x_test = data[data['label'].isnull()]\n",
    "features_name = [i for i in data.columns if i not in ['id','positive_negtive','id_feature','regcap','reccap','enttype','enttypeitem','enttypeminu','bucket_regcap_reccap','townsign','count_opscope','opscope_1','oplocdistrict','adbusign','compform','opform','orgid','exenum','opscope','enttypegb','label','state','jobid','regtype','parnum','venind','protype','oploc','forreccap','forregcap','congro']]\n",
    "#features_name = ['tfidif_opscope', 'industryphy', 'industryco', 'dom', 'opfrom', 'opto', 'empnum', 'enttpe_year', 'opscope_length', 'opscope_label', 'regcap_reccap', 'bucket_regcap', 'bucket_reccap', 'enttypegb_enttypeitem', 'enttype_enttypegb_enttypeitem', 'industryphy_industryco', 'industryco_enttypeitem', 'enttypegb_enttypeitem_industryphy_industryco', 'industryphy_counts', 'enttype_counts', 'enttypeitem_counts', 'empnum_counts', 'opfrom_counts', 'regcap_counts', 'reccap_counts', 'opto_counts', 'enttypeminu_counts']\n",
    "#features_name = ['industryphy', 'industryco', 'dom', 'opfrom', 'opto', 'empnum', 'enttpe_year', 'opscope_length', 'opscope_label', 'regcap_reccap', 'bucket_regcap', 'bucket_reccap', 'enttypegb_enttypeitem', 'industryphy_industryco', 'enttypegb_industryphy', 'industryphy_enttypeitem', 'industryco_enttypeitem', 'enttypegb_enttypeitem_industryphy_industryco', 'industryphy_counts', 'enttype_counts', 'enttypeitem_counts', 'empnum_counts', 'opfrom_counts', 'regcap_counts', 'reccap_counts', 'opto_counts', 'enttypeminu_counts', 'id_feature_dom']\n",
    "train_data = x_train[features_name]\n",
    "test_data = x_test[features_name]\n",
    "# train_data = train_data.fillna(-1)\n",
    "# test_data = test_data.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14865 entries, 0 to 14864\n",
      "Data columns (total 20 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   industryphy                    14865 non-null  int64  \n",
      " 1   industryco                     14865 non-null  float64\n",
      " 2   dom                            14865 non-null  int64  \n",
      " 3   opfrom                         14865 non-null  int32  \n",
      " 4   opto                           14865 non-null  float64\n",
      " 5   empnum                         14865 non-null  float64\n",
      " 6   opscope_feature                14865 non-null  int64  \n",
      " 7   regcap_reccap                  14865 non-null  float64\n",
      " 8   bucket_regcap                  14865 non-null  int64  \n",
      " 9   bucket_reccap                  14865 non-null  int64  \n",
      " 10  enttypegb_enttypeitem          14865 non-null  int64  \n",
      " 11  enttype_enttypegb_enttypeitem  14865 non-null  int64  \n",
      " 12  industryphy_industryco         14865 non-null  int64  \n",
      " 13  enttypegb_industryphy          14865 non-null  int64  \n",
      " 14  industryphy_enttypeitem        14865 non-null  int64  \n",
      " 15  industryco_enttypeitem         14865 non-null  int64  \n",
      " 16  industryphy_counts             14865 non-null  int64  \n",
      " 17  enttype_counts                 14865 non-null  int64  \n",
      " 18  enttypeitem_counts             14865 non-null  int64  \n",
      " 19  enttypeminu_counts             14865 non-null  int64  \n",
      "dtypes: float64(4), int32(1), int64(15)\n",
      "memory usage: 2.3 MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_score(y_test,y_pre):\n",
    "    _,_,f_class,_=precision_recall_fscore_support(y_true=y_test,y_pred=y_pre,labels=[0,1],average=None)\n",
    "    fper_class={'合法':f_class[0],'违法':f_class[1],'f1':f1_score(y_test,y_pre)}\n",
    "    return fper_class\n",
    "#\n",
    "def k_fold_serachParmaters(model,train_val_data,train_val_kind):\n",
    "    mean_f1=0\n",
    "    mean_f1Train=0\n",
    "    n_splits=5\n",
    "    sk = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2020)\n",
    "    for train, test in sk.split(train_val_data, train_val_kind):\n",
    "        x_train = train_val_data.iloc[train]\n",
    "        y_train = train_val_kind.iloc[train]\n",
    "        x_test = train_val_data.iloc[test]\n",
    "        y_test = train_val_kind.iloc[test]\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "        pred = model.predict(x_test)\n",
    "        fper_class =  eval_score(y_test,pred)\n",
    "        mean_f1+=fper_class['f1']/n_splits\n",
    "        #print(fper_class)\n",
    "        pred_Train = model.predict(x_train)\n",
    "        fper_class_train =  eval_score(y_train,pred_Train)\n",
    "        mean_f1Train+=fper_class_train['f1']/n_splits\n",
    "    #print('mean valf1:',mean_f1)\n",
    "    #print('mean trainf1:',mean_f1Train)\n",
    "    return mean_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlf: 0.840379639061893\n",
      "llf: 0.841320641054037\n",
      "clf: 0.8453457117327261\n",
      "rf: 0.8466039186976277\n"
     ]
    }
   ],
   "source": [
    "xlf=xgb.XGBClassifier(max_depth=5\n",
    "                      ,learning_rate=0.05\n",
    "                      ,n_estimators=150\n",
    "                      ,reg_alpha=0.01\n",
    "                      ,n_jobs=5\n",
    "                      ,importance_type='total_cover'\n",
    "                     )\n",
    "#\n",
    "llf=lgb.LGBMClassifier(num_leaves=10\n",
    "                           ,max_depth=5\n",
    "                           ,min_child_samples=10\n",
    "                           ,learning_rate=0.03\n",
    "                           ,n_estimators=150\n",
    "                           ,n_jobs=-1\n",
    "                           )\n",
    "  \n",
    "clf=cab.CatBoostClassifier(iterations=500\n",
    "                              ,learning_rate=0.01\n",
    "                              ,depth=4\n",
    "                              ,l2_leaf_reg=3\n",
    "                              ,silent=True\n",
    "                              ,thread_count=8\n",
    "                              ,task_type='CPU'\n",
    "                              )\n",
    "\n",
    "rf = RandomForestClassifier(oob_score=True, random_state=2020,\n",
    "            n_estimators= 60,max_depth=13,min_samples_split=10)\n",
    "#k_fold_serachParmaters(rf,train_data,kind)\n",
    "print('xlf:',k_fold_serachParmaters(xlf,train_data,kind))\n",
    "print('llf:',k_fold_serachParmaters(llf,train_data,kind))\n",
    "print('clf:',k_fold_serachParmaters(clf,train_data,kind)) \n",
    "print('rf:',k_fold_serachParmaters(rf,train_data,kind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每1次验证的f1:0.8418367346938775\n",
      "每2次验证的f1:0.8693467336683417\n",
      "每3次验证的f1:0.85012285012285\n",
      "每4次验证的f1:0.8480392156862746\n",
      "每5次验证的f1:0.8512820512820514\n",
      "mean f1: 0.8521255170906791\n"
     ]
    }
   ],
   "source": [
    "details = []\n",
    "answers = []\n",
    "mean_f1=0\n",
    "n_splits=5\n",
    "sk = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2020)\n",
    "cnt=0\n",
    "for train, test in sk.split(train_data, kind):\n",
    "    x_train = train_data.iloc[train]\n",
    "    y_train = kind.iloc[train]\n",
    "    x_test = train_data.iloc[test]\n",
    "    y_test = kind.iloc[test]\n",
    "\n",
    "    xlf.fit(x_train, y_train)\n",
    "    pred_xgb = xlf.predict(x_test)\n",
    "    weight_xgb = eval_score(y_test,pred_xgb)['f1']\n",
    "\n",
    "    llf.fit(x_train, y_train)\n",
    "    pred_llf = llf.predict(x_test)\n",
    "    weight_lgb = eval_score(y_test,pred_llf)['f1']\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred_cab = clf.predict(x_test)\n",
    "    weight_cab =  eval_score(y_test,pred_cab)['f1']\n",
    "\n",
    "    rf.fit(x_train, y_train)\n",
    "    pred_rf = rf.predict(x_test)\n",
    "    weight_rf =  eval_score(y_test,pred_rf)['f1']\n",
    "\n",
    "\n",
    "    prob_xgb = xlf.predict_proba(x_test)\n",
    "    prob_lgb = llf.predict_proba(x_test)\n",
    "    prob_cab = clf.predict_proba(x_test)\n",
    "    prob_rf = rf.predict_proba(x_test)\n",
    "\n",
    "    scores = []\n",
    "    ijkl = []\n",
    "    weight = np.arange(0.1, 1.05, 0.1)\n",
    "    for i, item1 in enumerate(weight):\n",
    "        for j, item2 in enumerate(weight[weight <= (1 - item1)]):\n",
    "            for k, item3 in enumerate(weight[weight <= (1 - item1-item2)]):\n",
    "                prob_end = prob_xgb * item1 + prob_lgb * item2 + prob_cab *item3+prob_rf*(1 - item1 - item2-item3)\n",
    "                #prob_end = np.sqrt(prob_xgb**2 * item1 + prob_lgb**2 * item2 + prob_cab**2 *item3+prob_rf**2*(1 - item1 - item2-item3))\n",
    "                score = eval_score(y_test,np.argmax(prob_end,axis=1))['f1']\n",
    "                scores.append(score)\n",
    "                ijkl.append((item1, item2,item3, 1 - item1 - item2-item3))\n",
    "    ii = ijkl[np.argmax(scores)][0]\n",
    "    jj = ijkl[np.argmax(scores)][1]\n",
    "    kk = ijkl[np.argmax(scores)][2]\n",
    "    ll = ijkl[np.argmax(scores)][3]\n",
    "\n",
    "    details.append(max(scores))\n",
    "    details.append(weight_xgb)\n",
    "    details.append(weight_lgb)\n",
    "    details.append(weight_cab)\n",
    "    details.append(weight_rf)\n",
    "    details.append(ii)\n",
    "    details.append(jj)\n",
    "    details.append(kk)\n",
    "    details.append(ll)\n",
    "\n",
    "    cnt+=1\n",
    "    print('每{}次验证的f1:{}'.format(cnt,max(scores)))\n",
    "    mean_f1+=max(scores)/n_splits\n",
    "\n",
    "    test_xgb = xlf.predict_proba(test_data)\n",
    "    test_lgb = llf.predict_proba(test_data)\n",
    "    test_cab = clf.predict_proba(test_data)\n",
    "    test_rf = rf.predict_proba(test_data)\n",
    "    #加权平均\n",
    "    ans = test_xgb * ii + test_lgb * jj + test_cab * kk + test_rf*ll#加权平均\n",
    "    #加权平方平均\n",
    "    #ans = np.sqrt(test_xgb**2 * ii + test_lgb**2 * jj + test_cab**2 * kk + test_rf**2*ll)\n",
    "    answers.append(ans)\n",
    "print('mean f1:',mean_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_end_score</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>catboost</th>\n",
       "      <th>rf</th>\n",
       "      <th>weight_xgboost</th>\n",
       "      <th>weight_lightgbm</th>\n",
       "      <th>weight_catboost</th>\n",
       "      <th>weight_rf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.841837</td>\n",
       "      <td>0.827068</td>\n",
       "      <td>0.837563</td>\n",
       "      <td>0.840506</td>\n",
       "      <td>0.839695</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.861461</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.850123</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.838235</td>\n",
       "      <td>0.841076</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.848039</td>\n",
       "      <td>0.836186</td>\n",
       "      <td>0.835381</td>\n",
       "      <td>0.834568</td>\n",
       "      <td>0.847880</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.851282</td>\n",
       "      <td>0.824742</td>\n",
       "      <td>0.846939</td>\n",
       "      <td>0.851282</td>\n",
       "      <td>0.843188</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_end_score   xgboost  lightgbm  catboost        rf  weight_xgboost  \\\n",
       "0        0.841837  0.827068  0.837563  0.840506  0.839695             0.1   \n",
       "1        0.869347  0.870000  0.848485  0.859296  0.861461             0.2   \n",
       "2        0.850123  0.843902  0.838235  0.841076  0.840796             0.2   \n",
       "3        0.848039  0.836186  0.835381  0.834568  0.847880             0.1   \n",
       "4        0.851282  0.824742  0.846939  0.851282  0.843188             0.1   \n",
       "\n",
       "   weight_lightgbm  weight_catboost  weight_rf  \n",
       "0              0.1              0.1        0.7  \n",
       "1              0.1              0.6        0.1  \n",
       "2              0.1              0.1        0.6  \n",
       "3              0.1              0.5        0.3  \n",
       "4              0.1              0.6        0.2  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(np.array(details).reshape(int(len(details)/9),9)\n",
    "                ,columns=['test_end_score','xgboost','lightgbm','catboost','rf'\n",
    "                ,'weight_xgboost','weight_lightgbm','weight_catboost','weight_rf'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fina=sum(answers)/n_splits#\n",
    "#fina=np.sqrt(sum(np.array(answers)**2)/n_splits)#平方平均\n",
    "fina=fina[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f000950527a6feb670cc1c87c2025f3922aaa4a0206a0a33</td>\n",
       "      <td>0.693468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>82750f1b9d1223502cdde9ba782cad55e52842584fe34e78</td>\n",
       "      <td>0.794626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>e9f7b28ec10e047089da56e946badaf8478af24af225838e</td>\n",
       "      <td>0.658691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>59b38c56de383683fbf0391346de363a8c529780e7ba7156</td>\n",
       "      <td>0.721837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>f000950527a6feb63fbd71bde689bf93d6ce72edd71e5d8d</td>\n",
       "      <td>0.903259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9930</th>\n",
       "      <td>59b38c56de383683eeb1d417fd4e036250ba48383b6d9512</td>\n",
       "      <td>0.806848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9961</th>\n",
       "      <td>82750f1b9d122350fe43715671f7fe9751ce0db3f171d192</td>\n",
       "      <td>0.524074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>59b38c56de383683ebfcfa6be70d1d44b2827a2f3f518dce</td>\n",
       "      <td>0.798033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>f000950527a6feb624e4b51b194d181ae1d48ef3e8b8964b</td>\n",
       "      <td>0.809823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>f000950527a6feb6018eda95d4e4ae8cbdd85c30d3cd342a</td>\n",
       "      <td>0.893201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>935 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    id     score\n",
       "1     f000950527a6feb670cc1c87c2025f3922aaa4a0206a0a33  0.693468\n",
       "37    82750f1b9d1223502cdde9ba782cad55e52842584fe34e78  0.794626\n",
       "53    e9f7b28ec10e047089da56e946badaf8478af24af225838e  0.658691\n",
       "56    59b38c56de383683fbf0391346de363a8c529780e7ba7156  0.721837\n",
       "68    f000950527a6feb63fbd71bde689bf93d6ce72edd71e5d8d  0.903259\n",
       "...                                                ...       ...\n",
       "9930  59b38c56de383683eeb1d417fd4e036250ba48383b6d9512  0.806848\n",
       "9961  82750f1b9d122350fe43715671f7fe9751ce0db3f171d192  0.524074\n",
       "9972  59b38c56de383683ebfcfa6be70d1d44b2827a2f3f518dce  0.798033\n",
       "9990  f000950527a6feb624e4b51b194d181ae1d48ef3e8b8964b  0.809823\n",
       "9999  f000950527a6feb6018eda95d4e4ae8cbdd85c30d3cd342a  0.893201\n",
       "\n",
       "[935 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entprise_test = pd.read_csv('entprise_evaluate.csv')\n",
    "dict1 = {\n",
    "    'id':entprise_test.id,\n",
    "    'score':fina\n",
    "}\n",
    "df = pd.DataFrame(dict1)\n",
    "df[df['score']>=0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "entprise_test = pd.read_csv('entprise_evaluate.csv')\n",
    "dict1 = {\n",
    "    'id':entprise_test.id,\n",
    "    'score':fina\n",
    "}\n",
    "df = pd.DataFrame(dict1)\n",
    "df.to_csv(r'./result/1203_8.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
