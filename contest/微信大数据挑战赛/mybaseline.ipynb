{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=brown>赛事概要：（结构化：微信大数据挑战赛）</font>\n",
    "\n",
    "## 本次比赛基于脱敏和采样后的数据信息，对于给定的一定数量到访过微信视频号“热门推荐”的用户， 根据这些用户在视频号内的历史14天的行为数据，通过算法在测试集上预测出这些用户在第15天对于不同视频内容的互动行为（包括点赞、点击头像、收藏、转发等）的发生概率。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "from deepctr_torch.models.deepfm import *\n",
    "from deepctr_torch.models.basemodel import *\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "#from deepctr.models import DeepFM\n",
    "from deepctr_torch.models import DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储数据的根目录\n",
    "ROOT_PATH = \"F:/aliyuun_datas/微信大数据挑战赛\"\n",
    "# 比赛数据集路径\n",
    "DATASET_PATH = ROOT_PATH + '/wechat_algo_data1/'\n",
    "# 训练集\n",
    "USER_ACTION = DATASET_PATH + \"user_action.csv\"\n",
    "FEED_INFO = DATASET_PATH + \"feed_info.csv\"\n",
    "FEED_EMBEDDINGS = DATASET_PATH + \"feed_embeddings.csv\"\n",
    "# 测试集\n",
    "TEST_FILE = DATASET_PATH + \"test_a.csv\"\n",
    "\n",
    "# 初赛待预测行为列表\n",
    "\n",
    "# ACTION_LIST = 【是否查看评论、是否点赞、是否点击头像、是否转发】\n",
    "ACTION_LIST = [\"read_comment\", \"like\", \"click_avatar\", \"forward\"]\n",
    "# FEA_COLUMN_LIST = 【是否查看评论、是否点赞、是否点击头像、是否转发、是否发表评论、是否关注、是否收藏】\n",
    "FEA_COLUMN_LIST = [\"read_comment\", \"like\", \"click_avatar\", \"forward\", \"comment\", \"follow\", \"favorite\"]\n",
    "# FEA_FEED_LIST =【Feed视频ID、视频号作者ID、Feed时长、背景音乐ID、背景音乐歌手ID】\n",
    "FEA_FEED_LIST = ['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']\n",
    "\n",
    "\n",
    "# 负样本下采样比例(负样本:正样本)\n",
    "ACTION_SAMPLE_RATE = {\"read_comment\": 5, \"like\": 5, \"click_avatar\": 5, \"forward\": 10, \"comment\": 10, \"follow\": 10,\n",
    "                      \"favorite\": 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7317882, 13) (106444, 15) (106444, 2) (421985, 3)\n"
     ]
    }
   ],
   "source": [
    "user_action = pd.read_csv(USER_ACTION)\n",
    "feed_info = pd.read_csv(FEED_INFO)\n",
    "feed_embddings = pd.read_csv(FEED_EMBEDDINGS)\n",
    "test_file = pd.read_csv(TEST_FILE)\n",
    "\n",
    "print(user_action.shape, feed_info.shape, feed_embddings.shape, test_file.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=orangered>样本处理</font>\n",
    "### 1. 数据左连接，构建数据集\n",
    "### 2. 时长 log 化处理\n",
    "### 3. 去重，保留最后一个\n",
    "### 4. 采样\n",
    "\n",
    "结果：\n",
    "生成1张表（测试集）：\n",
    "* 1.test_data.csv\n",
    "\n",
    "生成4张表（训练集）：\n",
    "* 1.train_data_for_read_comment.csv\n",
    "* 2.train_data_for_like.csv\n",
    "* 3.train_data_for_click)avatar.csv\n",
    "* 4.train_data_for_forward.csv\n",
    "\n",
    "### 5. 缺失处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(FEED_INFO, USER_ACTION, FEA_COLUMN_LIST, FEED_EMBEDDINGS, TEST_FILE, FEA_FEED_LIST, ROOT_PATH, ACTION_LIST, ACTION_SAMPLE_RATE):\n",
    "    feed_info_df = pd.read_csv(FEED_INFO) # 训练集 feed_info.csv\n",
    "    user_action_df = pd.read_csv(USER_ACTION)[[\"userid\", \"date_\", \"feedid\"] + FEA_COLUMN_LIST]\n",
    "    feed_embed = pd.read_csv(FEED_EMBEDDINGS) # feed_embeddings.csv\n",
    "    test = pd.read_csv(TEST_FILE) # 测试集 test_a.csv\n",
    "    # add feed feature\n",
    "    train = pd.merge(user_action_df, feed_info_df[FEA_FEED_LIST], on='feedid', how='left') # 合并表\n",
    "    test = pd.merge(test, feed_info_df[FEA_FEED_LIST], on='feedid', how='left') \n",
    "    test[\"videoplayseconds\"] = np.log(test[\"videoplayseconds\"] + 1.0) # Feed时长log处理\n",
    "    test.to_csv(ROOT_PATH + f'/test_data.csv', index=False)\n",
    "    for action in tqdm(ACTION_LIST):#ACTION_LIST = [\"read_comment\", \"like\", \"click_avatar\", \"forward\"]\n",
    "        print(f\"prepare data for {action}\")\n",
    "        # 去除重复项，除了最后一次出现的。\n",
    "        tmp = train.drop_duplicates(['userid', 'feedid', action], keep='last')  \n",
    "        df_neg = tmp[tmp[action] == 0]#df_neg含义：没有查看评论、没有点赞、没有点击头像、没有转发\n",
    "        # pd.sample()实现从df中随机抽样,frac参数是抽取行的比例，replace参数是是否有放回的抽样False为无放回\n",
    "        df_neg = df_neg.sample(frac=1.0 / ACTION_SAMPLE_RATE[action], random_state=42, replace=False)\n",
    "        df_all = pd.concat([df_neg, tmp[tmp[action] == 1]])\n",
    "        # Feed时长log处理，避免离散的异常数值影响，同时缓解运算\n",
    "        df_all[\"videoplayseconds\"] = np.log(df_all[\"videoplayseconds\"] + 1.0) \n",
    "        df_all.to_csv(ROOT_PATH + f'/train_data_for_{action}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare data for read_comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:08<00:25,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare data for like\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:16<00:16,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare data for click_avatar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:24<00:08,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare data for forward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:29<00:00,  7.37s/it]\n"
     ]
    }
   ],
   "source": [
    "prepare_data(FEED_INFO, USER_ACTION, FEA_COLUMN_LIST, FEED_EMBEDDINGS, TEST_FILE, FEA_FEED_LIST, ROOT_PATH, ACTION_LIST, ACTION_SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=gree>CTR模型：DeepFM\n",
    "## <font color=bulle>模型采用以下特征：\n",
    "## <font color=orange>离散特征：'user_id', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id'\n",
    "## <font color=orange>连续特征：videoplayseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBaseModel(BaseModel):\n",
    "\n",
    "    # 训练\n",
    "    def fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, initial_epoch=0, validation_split=0.,\n",
    "            validation_data=None, shuffle=True, callbacks=None):\n",
    "\n",
    "        if isinstance(x, dict):\n",
    "            x = [x[feature] for feature in self.feature_index]\n",
    "\n",
    "        do_validation = False\n",
    "        if validation_data:\n",
    "            do_validation = True\n",
    "            if len(validation_data) == 2:\n",
    "                val_x, val_y = validation_data\n",
    "                val_sample_weight = None\n",
    "            elif len(validation_data) == 3:\n",
    "                val_x, val_y, val_sample_weight = validation_data  # pylint: disable=unpacking-non-sequence\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    'When passing a `validation_data` argument, '\n",
    "                    'it must contain either 2 items (x_val, y_val), '\n",
    "                    'or 3 items (x_val, y_val, val_sample_weights), '\n",
    "                    'or alternatively it could be a dataset or a '\n",
    "                    'dataset or a dataset iterator. '\n",
    "                    'However we received `validation_data=%s`' % validation_data)\n",
    "            if isinstance(val_x, dict):\n",
    "                val_x = [val_x[feature] for feature in self.feature_index]\n",
    "\n",
    "        elif validation_split and 0. < validation_split < 1.:\n",
    "            do_validation = True\n",
    "            if hasattr(x[0], 'shape'):\n",
    "                split_at = int(x[0].shape[0] * (1. - validation_split))\n",
    "            else:\n",
    "                split_at = int(len(x[0]) * (1. - validation_split))\n",
    "            x, val_x = (slice_arrays(x, 0, split_at),\n",
    "                        slice_arrays(x, split_at))\n",
    "            y, val_y = (slice_arrays(y, 0, split_at),\n",
    "                        slice_arrays(y, split_at))\n",
    "        else:\n",
    "            val_x = []\n",
    "            val_y = []\n",
    "        for i in range(len(x)):\n",
    "            if len(x[i].shape) == 1:\n",
    "                x[i] = np.expand_dims(x[i], axis=1)\n",
    "\n",
    "        train_tensor_data = Data.TensorDataset(\n",
    "            torch.from_numpy(\n",
    "                np.concatenate(x, axis=-1)),\n",
    "            torch.from_numpy(y))\n",
    "        if batch_size is None:\n",
    "            batch_size = 256\n",
    "\n",
    "        model = self.train()\n",
    "        loss_func = self.loss_func\n",
    "        optim = self.optim\n",
    "\n",
    "        if self.gpus:\n",
    "            print('parallel running on these gpus:', self.gpus)\n",
    "            model = torch.nn.DataParallel(model, device_ids=self.gpus)\n",
    "            batch_size *= len(self.gpus)  # input `batch_size` is batch_size per gpu\n",
    "        else:\n",
    "            print(self.device)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_tensor_data, shuffle=shuffle, batch_size=batch_size)\n",
    "\n",
    "        sample_num = len(train_tensor_data)\n",
    "        steps_per_epoch = (sample_num - 1) // batch_size + 1\n",
    "\n",
    "        # configure callbacks\n",
    "        callbacks = (callbacks or []) + [self.history]  # add history callback\n",
    "        callbacks = CallbackList(callbacks)\n",
    "        callbacks.on_train_begin()\n",
    "        callbacks.set_model(self)\n",
    "        if not hasattr(callbacks, 'model'):\n",
    "            callbacks.__setattr__('model', self)\n",
    "        callbacks.model.stop_training = False\n",
    "\n",
    "        # Train\n",
    "        print(\"Train on {0} samples, validate on {1} samples, {2} steps per epoch\".format(\n",
    "            len(train_tensor_data), len(val_y), steps_per_epoch))\n",
    "        for epoch in range(initial_epoch, epochs):\n",
    "            callbacks.on_epoch_begin(epoch)\n",
    "            epoch_logs = {}\n",
    "            start_time = time.time()\n",
    "            loss_epoch = 0\n",
    "            total_loss_epoch = 0\n",
    "            train_result = {}\n",
    "            try:\n",
    "                with tqdm(enumerate(train_loader), disable=verbose != 1) as t:\n",
    "                    for _, (x_train, y_train) in t:\n",
    "                        x = x_train.to(self.device).float()\n",
    "                        y = y_train.to(self.device).float()\n",
    "\n",
    "                        y_pred = model(x).squeeze()\n",
    "\n",
    "                        optim.zero_grad()\n",
    "                        loss = loss_func(y_pred, y.squeeze(), reduction='sum')\n",
    "                        reg_loss = self.get_regularization_loss()\n",
    "\n",
    "                        total_loss = loss + reg_loss + self.aux_loss\n",
    "\n",
    "                        loss_epoch += loss.item()\n",
    "                        total_loss_epoch += total_loss.item()\n",
    "                        total_loss.backward()\n",
    "                        optim.step()\n",
    "\n",
    "                        if verbose > 0:\n",
    "                            for name, metric_fun in self.metrics.items():\n",
    "                                if name not in train_result:\n",
    "                                    train_result[name] = []\n",
    "                                try:\n",
    "                                    temp = metric_fun(\n",
    "                                        y.cpu().data.numpy(), y_pred.cpu().data.numpy().astype(\"float64\"))\n",
    "                                except Exception:\n",
    "                                    temp = 0\n",
    "                                finally:\n",
    "                                    train_result[name].append(temp)\n",
    "            except KeyboardInterrupt:\n",
    "                t.close()\n",
    "                raise\n",
    "            t.close()\n",
    "\n",
    "            # Add epoch_logs\n",
    "            epoch_logs[\"loss\"] = total_loss_epoch / sample_num\n",
    "            for name, result in train_result.items():\n",
    "                epoch_logs[name] = np.sum(result) / steps_per_epoch\n",
    "\n",
    "            if do_validation:\n",
    "                eval_result = self.evaluate(val_x, val_y, batch_size)\n",
    "                for name, result in eval_result.items():\n",
    "                    epoch_logs[\"val_\" + name] = result\n",
    "            # verbose\n",
    "            if verbose > 0:\n",
    "                epoch_time = int(time.time() - start_time)\n",
    "                print('Epoch {0}/{1}'.format(epoch + 1, epochs))\n",
    "\n",
    "                eval_str = \"{0}s - loss: {1: .4f}\".format(\n",
    "                    epoch_time, epoch_logs[\"loss\"])\n",
    "\n",
    "                for name in self.metrics:\n",
    "                    eval_str += \" - \" + name + \\\n",
    "                                \": {0: .4f}\".format(epoch_logs[name])\n",
    "\n",
    "                if do_validation:\n",
    "                    for name in self.metrics:\n",
    "                        eval_str += \" - \" + \"val_\" + name + \\\n",
    "                                    \": {0: .4f}\".format(epoch_logs[\"val_\" + name])\n",
    "                print(eval_str)\n",
    "            callbacks.on_epoch_end(epoch, epoch_logs)\n",
    "            if self.stop_training:\n",
    "                break\n",
    "\n",
    "        callbacks.on_train_end()\n",
    "\n",
    "        return self.history\n",
    "\n",
    "    # 评估\n",
    "    def evaluate(self, x, y, batch_size=256):\n",
    "        \"\"\"\n",
    "        ：param x:Numpy测试数据数组（如果模型有一个输入），或Numpy数组列表（如果模型有多个输入）。\n",
    "        ：param y:Numpy目标（标签）数据数组（如果模型有一个输出），或Numpy数组列表（如果模型有多个输出）。\n",
    "        ：param batch_size:整数或“无”。每个评估步骤的样本数。如果未指定，“批次大小”将默认为256。\n",
    "        ：return:Dict包含度量名称和度量值。\n",
    "        \"\"\"\n",
    "        pred_ans = self.predict(x, batch_size)\n",
    "        eval_result = {}\n",
    "        for name, metric_fun in self.metrics.items():\n",
    "            try:\n",
    "                temp = metric_fun(y, pred_ans)\n",
    "            except Exception:\n",
    "                temp = 0\n",
    "            finally:\n",
    "                eval_result[name] = metric_fun(y, pred_ans)\n",
    "        return eval_result\n",
    "\n",
    "    # 预测\n",
    "    def predict(self, x, batch_size=256):\n",
    "        \"\"\"\n",
    "        ：param x：输入数据，作为Numpy数组（如果模型有多个输入，则为Numpy数组列表）。\n",
    "        ：param batch_size:整数。如果未指定，则默认为256。\n",
    "        ：return:Numpy预测数组。\n",
    "        \"\"\"\n",
    "        model = self.eval()\n",
    "        if isinstance(x, dict):\n",
    "            x = [x[feature] for feature in self.feature_index]\n",
    "        for i in range(len(x)):\n",
    "            if len(x[i].shape) == 1:\n",
    "                x[i] = np.expand_dims(x[i], axis=1)\n",
    "\n",
    "        tensor_data = Data.TensorDataset(\n",
    "            torch.from_numpy(np.concatenate(x, axis=-1)))\n",
    "        test_loader = DataLoader(\n",
    "            dataset=tensor_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "        pred_ans = []\n",
    "        with torch.no_grad():\n",
    "            for _, x_test in enumerate(test_loader):\n",
    "                x = x_test[0].to(self.device).float()\n",
    "\n",
    "                y_pred = model(x).cpu().data.numpy()  # .squeeze()\n",
    "                pred_ans.append(y_pred)\n",
    "\n",
    "        return np.concatenate(pred_ans).astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDeepFM(MyBaseModel):\n",
    "    def __init__(self,\n",
    "                 linear_feature_columns, dnn_feature_columns, use_fm=True,\n",
    "                 dnn_hidden_units=(256, 128),\n",
    "                 l2_reg_linear=0.00001, l2_reg_embedding=0.00001, l2_reg_dnn=0, init_std=0.0001, seed=1024,\n",
    "                 dnn_dropout=0,\n",
    "                 dnn_activation='relu', dnn_use_bn=False, task='binary', device='cpu', gpus=None):\n",
    "\n",
    "        # 调用父类构造函数，初始化模型参数\n",
    "        super(MyDeepFM, self).__init__(linear_feature_columns, dnn_feature_columns, l2_reg_linear=l2_reg_linear,\n",
    "                                     l2_reg_embedding=l2_reg_embedding, init_std=init_std, seed=seed, task=task,\n",
    "                                     device=device, gpus=gpus)\n",
    "\n",
    "        self.use_fm = use_fm\n",
    "        self.use_dnn = len(dnn_feature_columns) > 0 and len(\n",
    "            dnn_hidden_units) > 0\n",
    "        if use_fm:\n",
    "            self.fm = FM()\n",
    "\n",
    "        if self.use_dnn:\n",
    "            self.dnn = DNN(self.compute_input_dim(dnn_feature_columns), dnn_hidden_units,\n",
    "                           activation=dnn_activation, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout, use_bn=dnn_use_bn,\n",
    "                           init_std=init_std, device=device)\n",
    "            self.dnn_linear = nn.Linear(\n",
    "                dnn_hidden_units[-1], 1, bias=False).to(device)\n",
    "\n",
    "            self.add_regularization_weight(\n",
    "                filter(lambda x: 'weight' in x[0] and 'bn' not in x[0], self.dnn.named_parameters()), l2=l2_reg_dnn)\n",
    "            self.add_regularization_weight(self.dnn_linear.weight, l2=l2_reg_dnn)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        sparse_embedding_list, dense_value_list = self.input_from_feature_columns(X, self.dnn_feature_columns,\n",
    "                                                                                  self.embedding_dict)\n",
    "        logit = self.linear_model(X)\n",
    "\n",
    "        if self.use_fm and len(sparse_embedding_list) > 0:\n",
    "            fm_input = torch.cat(sparse_embedding_list, dim=1)\n",
    "            logit += self.fm(fm_input)\n",
    "\n",
    "        if self.use_dnn:\n",
    "            dnn_input = combined_dnn_input(\n",
    "                sparse_embedding_list, dense_value_list)\n",
    "            dnn_output = self.dnn(dnn_input)\n",
    "            dnn_logit = self.dnn_linear(dnn_output)\n",
    "            logit += dnn_logit\n",
    "\n",
    "        y_pred = self.out(logit)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=yellow>待预测行为列表</font>\n",
    "### <font color=orange>ACTION_LIST = 【是否查看评论、是否点赞、是否点击头像、是否转发】\n",
    "### ACTION_LIST = [\"read_comment\", \"like\", \"click_avatar\", \"forward\"]\n",
    "### FEA_COLUMN_LIST = 【是否查看评论、是否点赞、是否点击头像、是否转发、是否发表评论、是否关注、是否收藏】\n",
    "### FEA_COLUMN_LIST = [\"read_comment\", \"like\", \"click_avatar\", \"forward\", \"comment\", \"follow\", \"favorite\"]\n",
    "### EA_FEED_LIST =【Feed视频ID、视频号作者ID、Feed时长、背景音乐ID、背景音乐歌手ID】\n",
    "### FEA_FEED_LIST = ['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']\n",
    "\n",
    "\n",
    "* <font color=grep>FM：\n",
    "    * 相当于对逻辑回归做了改进，加了特征交叉；目的是解决数据稀疏的情况下，特征怎样组合的问题。\n",
    "    * 主要应用场景是点击率预估，目的是在数据高维稀疏的情况下，解决特征的组合问题。\n",
    "*\n",
    "\n",
    "* DNN基本结构：\n",
    "    * 感知机：若干输入和一个输出，输入和输出是线性关系，还有一个神经元激活函数，此模型只能用于二分类问题。\n",
    "    * 神经网络：（1）加入隐藏层，增强模型表达能力。（2）多个输出。（3）激活函数的扩展。\n",
    "    * 神经网络是基于感知机的扩展，DNN可以理解为有很多隐藏层的神经网络。多层神经网络和DNN其实也指的一个东西，DNN有时也叫多层感知机。\n",
    "* \n",
    "\n",
    "* <font color=grep>DeepFM：将深度神经网络模型与FM模型结合，同时学习低阶和高阶的特征交叉，主要由FM和DNN两部分组成，底部共享同样的输入。FM模型善于挖掘二阶特征交叉关系，而神经网络DNN的优点是能够挖掘高阶的特征交叉关系，于是DeepFM将两者组合到一起，实验证明DeepFM比单模型FM、DNN效果好。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posi prop:\n",
      "0.15565121499983292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 19.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Train on 1316708 samples, validate on 329177 samples, 2572 steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2572it [01:17, 33.00it/s]\n",
      "2it [00:00, 17.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "79s - loss:  0.2507 - binary_crossentropy:  0.2351 - auc:  0.9227 - val_binary_crossentropy:  0.2167 - val_auc:  0.9392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2572it [01:13, 34.99it/s]\n",
      "2it [00:00, 15.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "75s - loss:  0.2211 - binary_crossentropy:  0.2075 - auc:  0.9456 - val_binary_crossentropy:  0.2142 - val_auc:  0.9408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2572it [01:14, 34.51it/s]\n",
      "2it [00:00, 16.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "76s - loss:  0.2137 - binary_crossentropy:  0.2018 - auc:  0.9491 - val_binary_crossentropy:  0.2145 - val_auc:  0.9408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2572it [01:14, 34.55it/s]\n",
      "2it [00:00, 17.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "76s - loss:  0.2092 - binary_crossentropy:  0.1983 - auc:  0.9512 - val_binary_crossentropy:  0.2156 - val_auc:  0.9404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2572it [01:14, 34.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "76s - loss:  0.2059 - binary_crossentropy:  0.1957 - auc:  0.9527 - val_binary_crossentropy:  0.2162 - val_auc:  0.9400\n",
      "posi prop:\n",
      "0.11860030306914048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 18.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Train on 1273372 samples, validate on 318344 samples, 2488 steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2488it [01:19, 31.35it/s]\n",
      "2it [00:00, 16.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "81s - loss:  0.2807 - binary_crossentropy:  0.2648 - auc:  0.8428 - val_binary_crossentropy:  0.2478 - val_auc:  0.8677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2488it [01:08, 36.57it/s]\n",
      "2it [00:00, 16.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "69s - loss:  0.2556 - binary_crossentropy:  0.2411 - auc:  0.8793 - val_binary_crossentropy:  0.2466 - val_auc:  0.8692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2488it [01:07, 36.82it/s]\n",
      "2it [00:00, 17.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "69s - loss:  0.2488 - binary_crossentropy:  0.2361 - auc:  0.8863 - val_binary_crossentropy:  0.2467 - val_auc:  0.8692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2488it [01:06, 37.34it/s]\n",
      "2it [00:00, 17.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "68s - loss:  0.2444 - binary_crossentropy:  0.2328 - auc:  0.8907 - val_binary_crossentropy:  0.2475 - val_auc:  0.8683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2488it [01:07, 37.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "69s - loss:  0.2411 - binary_crossentropy:  0.2302 - auc:  0.8942 - val_binary_crossentropy:  0.2485 - val_auc:  0.8674\n",
      "posi prop:\n",
      "0.0371053151111731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 19.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Train on 1187301 samples, validate on 296826 samples, 2319 steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2319it [01:15, 30.62it/s]\n",
      "2it [00:00, 16.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "77s - loss:  0.1379 - binary_crossentropy:  0.1269 - auc:  0.8270 - val_binary_crossentropy:  0.1165 - val_auc:  0.8675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2319it [01:06, 34.88it/s]\n",
      "2it [00:00, 18.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "68s - loss:  0.1190 - binary_crossentropy:  0.1089 - auc:  0.8969 - val_binary_crossentropy:  0.1156 - val_auc:  0.8717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2319it [01:04, 35.73it/s]\n",
      "2it [00:00, 17.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "66s - loss:  0.1127 - binary_crossentropy:  0.1040 - auc:  0.9118 - val_binary_crossentropy:  0.1163 - val_auc:  0.8708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2319it [01:05, 35.45it/s]\n",
      "2it [00:00, 16.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "67s - loss:  0.1089 - binary_crossentropy:  0.1009 - auc:  0.9210 - val_binary_crossentropy:  0.1174 - val_auc:  0.8694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2319it [01:05, 35.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "67s - loss:  0.1061 - binary_crossentropy:  0.0987 - auc:  0.9269 - val_binary_crossentropy:  0.1188 - val_auc:  0.8675\n",
      "posi prop:\n",
      "0.03752907526887493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 35.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Train on 596039 samples, validate on 149010 samples, 1165 steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1165it [00:37, 31.05it/s]\n",
      "3it [00:00, 28.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "38s - loss:  0.1355 - binary_crossentropy:  0.1248 - auc:  0.8402 - val_binary_crossentropy:  0.1086 - val_auc:  0.8954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1165it [00:32, 36.33it/s]\n",
      "3it [00:00, 27.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "32s - loss:  0.1076 - binary_crossentropy:  0.0963 - auc:  0.9329 - val_binary_crossentropy:  0.1073 - val_auc:  0.9005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1165it [00:32, 36.33it/s]\n",
      "3it [00:00, 27.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "32s - loss:  0.0982 - binary_crossentropy:  0.0881 - auc:  0.9496 - val_binary_crossentropy:  0.1092 - val_auc:  0.8998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1165it [00:31, 36.43it/s]\n",
      "3it [00:00, 28.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "32s - loss:  0.0922 - binary_crossentropy:  0.0829 - auc:  0.9582 - val_binary_crossentropy:  0.1128 - val_auc:  0.8969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1165it [00:31, 36.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "32s - loss:  0.0880 - binary_crossentropy:  0.0794 - auc:  0.9633 - val_binary_crossentropy:  0.1159 - val_auc:  0.8949\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    submit = pd.read_csv(ROOT_PATH + '/test_data.csv')[['userid', 'feedid']]\n",
    "    for action in ACTION_LIST:  # 预测行为列表[\"read_comment\", \"like\", \"click_avatar\", \"forward\"]\n",
    "        USE_FEAT = ['userid', 'feedid', action] + FEA_FEED_LIST[1:]\n",
    "        train = pd.read_csv(ROOT_PATH + f'/train_data_for_{action}.csv')[USE_FEAT]\n",
    "        train = train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        print(\"posi prop:\")  \n",
    "        print(sum((train[action]==1)*1)/train.shape[0]) # 1值占比情况\n",
    "        test = pd.read_csv(ROOT_PATH + '/test_data.csv')[[i for i in USE_FEAT if i != action]]\n",
    "        # [['userid', 'feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']]\n",
    "\n",
    "        target = [action]\n",
    "        test[target[0]] = 0\n",
    "        test = test[USE_FEAT]\n",
    "        data = pd.concat((train, test)).reset_index(drop=True)\n",
    "        dense_features = ['videoplayseconds']\n",
    "        sparse_features = [i for i in USE_FEAT if i not in dense_features and i not in target]\n",
    "        # 稀疏特征sparse_features = ['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id']\n",
    "\n",
    "        data[sparse_features] = data[sparse_features].fillna(0) # 缺失值处理\n",
    "        data[dense_features] = data[dense_features].fillna(0)\n",
    "\n",
    "        # 1.对稀疏特征进行标签编码，对稠密特征进行0-1标准化缩放。稀疏：大部分都是0值\n",
    "        for feat in sparse_features:\n",
    "            lbe = LabelEncoder()\n",
    "            data[feat] = lbe.fit_transform(data[feat])\n",
    "        mms = MinMaxScaler(feature_range=(0, 1))\n",
    "        data[dense_features] = mms.fit_transform(data[dense_features])\n",
    "\n",
    "        # 2.计算每个稀疏特征的不同值个数，并记录字段名\n",
    "        # 这里是比较关键的一步，因为需要对类别特征进行Embedding，\n",
    "        # 所以需要告诉模型一个特征组有多少个embbedding向量。（nunique）\n",
    "        fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique())\n",
    "                                  for feat in sparse_features] + [DenseFeat(feat, 1, )\n",
    "                                                                  for feat in dense_features]\n",
    "        dnn_feature_columns = fixlen_feature_columns\n",
    "        linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "        feature_names = get_feature_names(\n",
    "            linear_feature_columns + dnn_feature_columns)\n",
    "        # feature_names = ['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id', 'videoplayseconds']\n",
    "\n",
    "        # 3.为模型生成输入数据\n",
    "        train, test = data.iloc[:train.shape[0]].reset_index(drop=True), data.iloc[train.shape[0]:].reset_index(drop=True)\n",
    "        train_model_input = {name: train[name] for name in feature_names}\n",
    "        test_model_input = {name: test[name] for name in feature_names}\n",
    "\n",
    "        # 4.定义模型，训练，预测，评估\n",
    "        # 检查是否可以使用GPU\n",
    "        device = 'cpu'\n",
    "        use_cuda = True \n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            print('cuda ready...')\n",
    "            device = 'cuda:0'\n",
    "\n",
    "        # 初始化模型\n",
    "        model = MyDeepFM(linear_feature_columns=linear_feature_columns, dnn_feature_columns=dnn_feature_columns,\n",
    "                       task='binary',\n",
    "                       l2_reg_embedding=1e-1, device=device)\n",
    "\n",
    "        model.compile(\"adagrad\", \"binary_crossentropy\", metrics=[\"binary_crossentropy\", \"auc\"])\n",
    "\n",
    "        history = model.fit(train_model_input, train[target].values, \n",
    "                            batch_size=512, # 定义每次训练的批量数（整数型），默认为32 \n",
    "                            epochs=5,   # 训练模型的次数\n",
    "                            verbose=1,  # 日志显示，1表示输出进度条记录\n",
    "                            validation_split=0.2    # 划分验证集，数据集中序列靠后的数据进行划分\n",
    "                            )\n",
    "        pred_ans = model.predict(test_model_input, 128)\n",
    "        submit[action] = pred_ans\n",
    "        torch.cuda.empty_cache()\n",
    "    # 保存提交文件\n",
    "    # submit.to_csv(\"./submit_base_deepfm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5fb527b06e16c0bf53bd22ca6a0a890a2219c4494de699106f117a6dd9aed55e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
